{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpA7na957S4abMnliygDbe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoCodeProgram/deepLearning/blob/main/transformer/name_gender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NoCodeProgram/deepLearning.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJSYQNc8uQDM",
        "outputId": "6d05dd74-efb1-48c6-adac-ff9c98cf42aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deepLearning'...\n",
            "remote: Enumerating objects: 266, done.\u001b[K\n",
            "remote: Counting objects: 100% (127/127), done.\u001b[K\n",
            "remote: Compressing objects: 100% (125/125), done.\u001b[K\n",
            "remote: Total 266 (delta 48), reused 0 (delta 0), pack-reused 139\u001b[K\n",
            "Receiving objects: 100% (266/266), 12.41 MiB | 20.76 MiB/s, done.\n",
            "Resolving deltas: 100% (84/84), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVND2ZpEuIIS",
        "outputId": "0a2b80d2-c66b-460d-b089-0bf3711df173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2.1+cu121\n",
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "if torch.backends.mps.is_available():\n",
        "    my_device = torch.device('mps')\n",
        "elif torch.cuda.is_available():\n",
        "    my_device = torch.device('cuda')\n",
        "else:\n",
        "    my_device = torch.device('cpu')\n",
        "print(my_device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('./deepLearning/rnn/name_gender_filtered.csv')\n",
        "\n",
        "unique_chars = set()\n",
        "\n",
        "for name in df['Name']:\n",
        "    unique_chars.update(name)\n",
        "sorted_chars = sorted(list(unique_chars))\n",
        "\n"
      ],
      "metadata": {
        "id": "-w1iaYP8uRqC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_chars = sorted(set(''.join(sorted_chars)))\n",
        "stoi = {s:i for i,s in enumerate(sorted_chars)}\n",
        "stoi['<P>'] = len(stoi) #padding tokken\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "print(stoi)\n",
        "print(itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyMhQnXruTz0",
        "outputId": "4a97f6eb-a863-409c-e838-081a00f2d51a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25, '<P>': 26}\n",
            "{0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 9: 'j', 10: 'k', 11: 'l', 12: 'm', 13: 'n', 14: 'o', 15: 'p', 16: 'q', 17: 'r', 18: 's', 19: 't', 20: 'u', 21: 'v', 22: 'w', 23: 'x', 24: 'y', 25: 'z', 26: '<P>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "char_length = 16\n",
        "def encode_name(name):\n",
        "    name = [stoi[s] for s in name]\n",
        "    name += [stoi['<P>']]*(char_length-len(name))\n",
        "    return name\n",
        "\n",
        "def decode_name(name):\n",
        "    decoded_chars = [itos[i] for i in name if itos[i] != '<P>']\n",
        "    return ''.join(decoded_chars)\n",
        "\n",
        "print(encode_name('nocope'))\n",
        "print(decode_name(encode_name('nocope')))\n",
        "\n",
        "gen2num = {'F':0, 'M':1}\n",
        "num2gen = {0:'F', 1:'M'}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV9iAGfUuV69",
        "outputId": "26cf3b86-de61-4727-b9af-7d4bc783d1ce"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13, 14, 2, 14, 15, 4, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26]\n",
            "nocope\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_length = 16\n",
        "n_embed = 32\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, atten_dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(embed_dim, atten_dim, bias=False)\n",
        "        self.key = nn.Linear(embed_dim, atten_dim, bias=False)\n",
        "        self.value = nn.Linear(embed_dim, atten_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        query = self.query(x)\n",
        "        key = self.key(x)\n",
        "        value = self.value(x)\n",
        "\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1))\n",
        "        scores = scores / key.size(-1)**0.5\n",
        "\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        weighted_values = torch.matmul(attention_weights, value)\n",
        "\n",
        "        return weighted_values\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        attention_dim = embed_dim // num_heads\n",
        "        self.attentions = nn.ModuleList([SelfAttention(embed_dim, attention_dim) for _ in range(num_heads)])\n",
        "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        head_outputs = []\n",
        "        for attention in self.attentions:\n",
        "            head_output = attention(x)\n",
        "            head_outputs.append(head_output)\n",
        "\n",
        "        concatenated_heads = torch.cat(head_outputs, dim=-1)\n",
        "        output = self.fc(concatenated_heads)\n",
        "        return output\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, n_head):\n",
        "        super().__init__()\n",
        "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.multihead_atten = MultiheadAttention(embed_dim, n_head)\n",
        "\n",
        "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.feed_forward = FeedFoward(embed_dim, 4*embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.multihead_atten(self.layer_norm1(x))\n",
        "        x = x + self.feed_forward(self.layer_norm2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "uGRXO-89uY2R"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TransformerNameGenderClassifier(nn.Module):\n",
        "    def __init__(self, char_size, embed_dim, n_heads, n_layers, max_len, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.char_embedding = nn.Embedding(char_size, embed_dim)\n",
        "        self.positional_encoding = nn.Embedding(max_len, embed_dim)\n",
        "        self.transformer_blocks = nn.Sequential(*[TransformerBlock(embed_dim, n_heads) for _ in range(n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(embed_dim)\n",
        "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        char_embeddings = self.char_embedding(x)  # [batch_size, seq_length, embed_dim]\n",
        "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)  # [1, seq_length]\n",
        "        pos_embeddings = self.positional_encoding(positions)  # [1, seq_length, embed_dim]\n",
        "        x = char_embeddings + pos_embeddings\n",
        "        x = self.transformer_blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        x = x.mean(dim=1)\n",
        "        logits = self.classifier(x)\n",
        "        return logits\n",
        "\n",
        "# Parameters\n",
        "char_size = len(stoi)\n",
        "max_len = char_length  # Max length of name\n",
        "\n",
        "model = TransformerNameGenderClassifier(char_size=char_size, embed_dim=n_embed, n_heads=n_head, n_layers=n_layer, max_len=max_len)\n"
      ],
      "metadata": {
        "id": "e6HySz_ZundO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_batch(df, batch_size):\n",
        "    # Randomly sample a batch of data\n",
        "    batch = df.sample(n=batch_size)\n",
        "    names = batch['Name'].values\n",
        "    genders = batch['Gender'].values\n",
        "\n",
        "    # Encode names and genders\n",
        "    encoded_names = np.array([encode_name(name) for name in names])\n",
        "    encoded_genders = np.array([gen2num[gender] for gender in genders])\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    names_tensor = torch.tensor(encoded_names, dtype=torch.long)\n",
        "    genders_tensor = torch.tensor(encoded_genders, dtype=torch.long)\n",
        "\n",
        "    return names_tensor, genders_tensor\n",
        "\n",
        "# Example usage:\n",
        "batch_size = 4\n",
        "names_tensor, genders_tensor = get_batch(df, batch_size)\n",
        "print(\"Names Tensor:\", names_tensor)\n",
        "print(\"Genders Tensor:\", genders_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVJQWt7VuqYg",
        "outputId": "320b5b41-9089-42bd-95a7-1db8762cd33a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Names Tensor: tensor([[10,  4, 13,  0,  3,  8, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26],\n",
            "        [ 0, 17,  8, 18, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26],\n",
            "        [18, 19, 17, 24, 10,  4, 17, 26, 26, 26, 26, 26, 26, 26, 26, 26],\n",
            "        [ 4, 21,  0, 13,  6,  4, 11, 14, 18, 26, 26, 26, 26, 26, 26, 26]])\n",
            "Genders Tensor: tensor([0, 1, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Assuming df is your DataFrame and the TransformerNameGenderClassifier is defined and ready.\n",
        "\n",
        "# Model parameters\n",
        "vocab_size = len(stoi)  # Number of unique characters\n",
        "embed_dim = 32  # Size of character embeddings\n",
        "n_heads = 4  # Number of attention heads\n",
        "n_layers = 4  # Number of transformer blocks\n",
        "max_len = char_length  # Maximum length of a name\n",
        "num_classes = 2  # Gender classes: F or M\n",
        "\n",
        "# Instantiate the model\n",
        "model = TransformerNameGenderClassifier(vocab_size, embed_dim, n_heads, n_layers, max_len, num_classes)\n",
        "model.to(my_device)\n",
        "model.train()  # Set the model to training mode\n",
        "\n",
        "# Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training parameters\n",
        "epochs = 50\n",
        "batch_size = 64\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for _ in range(len(df) // batch_size):\n",
        "        # Get a batch of data\n",
        "        names_tensor, genders_tensor = get_batch(df, batch_size)\n",
        "        names_tensor = names_tensor.to(my_device)\n",
        "        genders_tensor = genders_tensor.to(my_device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(names_tensor)\n",
        "\n",
        "        # Compute and print loss\n",
        "        loss = criterion(predictions, genders_tensor)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass: Compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Calling the step function on an Optimizer makes an update to its parameters\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    print(f'Epoch {epoch+1}, Loss: {total_loss / (len(df) // batch_size)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "n0qvnrWBux6l",
        "outputId": "a15597fe-a834-45bc-8987-892dc3b4c7cd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.5207702869342433\n",
            "Epoch 2, Loss: 0.3988763694134023\n",
            "Epoch 3, Loss: 0.3855010283490022\n",
            "Epoch 4, Loss: 0.35684975516051054\n",
            "Epoch 5, Loss: 0.3401740736121105\n",
            "Epoch 6, Loss: 0.3234824066878193\n",
            "Epoch 7, Loss: 0.30880234447411364\n",
            "Epoch 8, Loss: 0.30198006404356825\n",
            "Epoch 9, Loss: 0.2771549000301295\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7d187caf16f3>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Backward pass: Compute gradient of the loss with respect to model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Calling the step function on an Optimizer makes an update to its parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model.to(torch.device('cpu'))\n",
        "\n",
        "# Encode the name and add an extra batch dimension\n",
        "names_tensor = torch.tensor(encode_name(\"nocope\"), dtype=torch.long)[None, :]\n",
        "\n",
        "# Perform the prediction\n",
        "with torch.no_grad():\n",
        "    pred = model(names_tensor)\n",
        "\n",
        "predicted_index = pred.argmax(1).item()\n",
        "print(f\"Predicted class: {num2gen[predicted_index]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JodL0cxVu0sn",
        "outputId": "75b8053b-b6ea-41b3-9036-919efde2453f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Use Torch encoder\n",
        "\n",
        "class TransformerNameGenderClassifier(nn.Module):\n",
        "    def __init__(self, char_size, embed_dim, n_heads, n_layers, max_len, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.char_embedding = nn.Embedding(char_size, embed_dim)\n",
        "        self.positional_encoding = nn.Embedding(max_len, embed_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, n_heads, dim_feedforward=4 * embed_dim, batch_first=True, norm_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(embed_dim)\n",
        "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        char_embeddings = self.char_embedding(x)  # [batch_size, seq_length, embed_dim]\n",
        "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)  # [1, seq_length]\n",
        "        pos_embeddings = self.positional_encoding(positions)  # [1, seq_length, embed_dim]\n",
        "        x = char_embeddings + pos_embeddings\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.ln_f(x)\n",
        "        x = x.mean(dim=1)\n",
        "        logits = self.classifier(x)\n",
        "        return logits\n",
        "\n",
        "# Parameters\n",
        "char_size = len(stoi)\n",
        "max_len = char_length  # Max length of name\n",
        "\n",
        "model = TransformerNameGenderClassifier(char_size=char_size, embed_dim=n_embed, n_heads=n_head, n_layers=n_layer, max_len=max_len)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA-ypzPivC4F",
        "outputId": "cd675eb2-5710-4957-dd0b-c3b4fa7611d2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    }
  ]
}