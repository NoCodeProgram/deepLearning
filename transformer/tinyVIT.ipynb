{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNfXBpq1IT9yOmGVgwz0aK8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NoCodeProgram/deepLearning/blob/main/transformer/tinyVIT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8gL4zPeVZCr",
        "outputId": "4ab90ef1-e138-40a5-93f6-c8873dbb8ce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    my_device = torch.device('mps')\n",
        "elif torch.cuda.is_available():\n",
        "    my_device = torch.device('cuda')\n",
        "else:\n",
        "    my_device = torch.device('cpu')\n",
        "\n",
        "print(my_device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Mfi8HPQVlya",
        "outputId": "465debbf-2a41-4b2b-db4e-1dfaa5604ed8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgYoP1KgVtNs",
        "outputId": "a1c68c0a-a12f-4d45-c989-815df71dd9c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 29.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, patch_size=4, in_channels=3, embed_dim=48):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_channels,\n",
        "            embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # (B, embed_dim, H/patch_size, W/patch_size)\n",
        "        x = x.flatten(2)  # (B, embed_dim, n_patches)\n",
        "        x = x.transpose(1, 2)  # (B, n_patches, embed_dim)\n",
        "        return x"
      ],
      "metadata": {
        "id": "oTe7fJMoVvHh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyViT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        img_size = 32\n",
        "        patch_size = 4\n",
        "        in_channels = 3\n",
        "        embed_dim = 48\n",
        "        num_heads = 4\n",
        "        dropout = 0.1\n",
        "        num_layers = 4\n",
        "        num_classes = 10\n",
        "        mlp_ratio = 4.0\n",
        "\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            patch_size=patch_size,\n",
        "            in_channels=in_channels,\n",
        "            embed_dim=embed_dim\n",
        "        )\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "        # Calculate number of patches for position embedding\n",
        "        n_patches = (img_size // patch_size) ** 2  # Assuming 32x32 input\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, embed_dim))\n",
        "\n",
        "        # Define encoder layer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=int(embed_dim * mlp_ratio),\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "\n",
        "        # Create transformer encoder\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "            nn.GELU(),  # or nn.ReLU()\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        x = self.norm(x)\n",
        "        x = x[:, 0]  # Take cls token\n",
        "        x = self.head(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "c-Tu2_75V2T-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TinyViT()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "model = model.to(my_device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OGGMQv1Xpkw",
        "outputId": "2136f0e2-2fe5-4f94-ccd6-f065d4453cb6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(my_device), labels.to(my_device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "\n",
        "    print(f'Epoch [{epoch+1}], Step [{i+1}/{len(train_loader)}], '\n",
        "            f'Loss: {running_loss/100:.4f}')\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(my_device), labels.to(my_device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atoSVdKlX7hf",
        "outputId": "bb8af5ff-a116-47ad-d412-1bf35b9429cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1], Step [391/391], Loss: 7.1850\n",
            "Test Accuracy: 39.71%\n",
            "Epoch [2], Step [391/391], Loss: 6.1883\n",
            "Test Accuracy: 45.55%\n",
            "Epoch [3], Step [391/391], Loss: 5.7046\n",
            "Test Accuracy: 48.06%\n",
            "Epoch [4], Step [391/391], Loss: 5.4545\n",
            "Test Accuracy: 50.41%\n",
            "Epoch [5], Step [391/391], Loss: 5.2948\n",
            "Test Accuracy: 51.14%\n",
            "Epoch [6], Step [391/391], Loss: 5.1815\n",
            "Test Accuracy: 53.66%\n",
            "Epoch [7], Step [391/391], Loss: 5.0526\n",
            "Test Accuracy: 54.08%\n",
            "Epoch [8], Step [391/391], Loss: 4.9464\n",
            "Test Accuracy: 54.93%\n",
            "Epoch [9], Step [391/391], Loss: 4.8681\n",
            "Test Accuracy: 56.37%\n",
            "Epoch [10], Step [391/391], Loss: 4.7644\n",
            "Test Accuracy: 56.55%\n",
            "Epoch [11], Step [391/391], Loss: 4.7315\n",
            "Test Accuracy: 56.17%\n",
            "Epoch [12], Step [391/391], Loss: 4.6344\n",
            "Test Accuracy: 57.39%\n",
            "Epoch [13], Step [391/391], Loss: 4.5733\n",
            "Test Accuracy: 57.11%\n",
            "Epoch [14], Step [391/391], Loss: 4.5229\n",
            "Test Accuracy: 57.76%\n",
            "Epoch [15], Step [391/391], Loss: 4.4779\n",
            "Test Accuracy: 59.87%\n",
            "Epoch [16], Step [391/391], Loss: 4.4258\n",
            "Test Accuracy: 58.63%\n",
            "Epoch [17], Step [391/391], Loss: 4.3679\n",
            "Test Accuracy: 59.17%\n",
            "Epoch [18], Step [391/391], Loss: 4.3175\n",
            "Test Accuracy: 60.37%\n",
            "Epoch [19], Step [391/391], Loss: 4.2775\n",
            "Test Accuracy: 58.25%\n",
            "Epoch [20], Step [391/391], Loss: 4.2327\n",
            "Test Accuracy: 61.78%\n",
            "Epoch [21], Step [391/391], Loss: 4.1821\n",
            "Test Accuracy: 61.76%\n",
            "Epoch [22], Step [391/391], Loss: 4.1391\n",
            "Test Accuracy: 61.42%\n",
            "Epoch [23], Step [391/391], Loss: 4.1168\n",
            "Test Accuracy: 62.09%\n",
            "Epoch [24], Step [391/391], Loss: 4.0496\n",
            "Test Accuracy: 62.10%\n",
            "Epoch [25], Step [391/391], Loss: 4.0239\n",
            "Test Accuracy: 61.83%\n",
            "Epoch [26], Step [391/391], Loss: 4.0172\n",
            "Test Accuracy: 62.35%\n",
            "Epoch [27], Step [391/391], Loss: 3.9478\n",
            "Test Accuracy: 63.60%\n",
            "Epoch [28], Step [391/391], Loss: 3.9282\n",
            "Test Accuracy: 62.43%\n",
            "Epoch [29], Step [391/391], Loss: 3.8912\n",
            "Test Accuracy: 62.94%\n",
            "Epoch [30], Step [391/391], Loss: 3.8353\n",
            "Test Accuracy: 63.35%\n",
            "Epoch [31], Step [391/391], Loss: 3.8040\n",
            "Test Accuracy: 63.19%\n",
            "Epoch [32], Step [391/391], Loss: 3.7756\n",
            "Test Accuracy: 64.65%\n",
            "Epoch [33], Step [391/391], Loss: 3.7565\n",
            "Test Accuracy: 63.71%\n",
            "Epoch [34], Step [391/391], Loss: 3.7113\n",
            "Test Accuracy: 64.41%\n",
            "Epoch [35], Step [391/391], Loss: 3.6944\n",
            "Test Accuracy: 64.45%\n",
            "Epoch [36], Step [391/391], Loss: 3.6440\n",
            "Test Accuracy: 65.30%\n",
            "Epoch [37], Step [391/391], Loss: 3.6097\n",
            "Test Accuracy: 64.90%\n",
            "Epoch [38], Step [391/391], Loss: 3.6060\n",
            "Test Accuracy: 65.39%\n",
            "Epoch [39], Step [391/391], Loss: 3.5453\n",
            "Test Accuracy: 66.09%\n",
            "Epoch [40], Step [391/391], Loss: 3.5296\n",
            "Test Accuracy: 65.58%\n",
            "Epoch [41], Step [391/391], Loss: 3.5004\n",
            "Test Accuracy: 65.64%\n",
            "Epoch [42], Step [391/391], Loss: 3.4768\n",
            "Test Accuracy: 66.33%\n"
          ]
        }
      ]
    }
  ]
}